\documentclass[reprint, english,notitlepage,nofootinbib]{revtex4-1}  % defines the basic parameters of the document
% if you want a single-column, remove reprint

% allows special characters (including æøå)
\usepackage[utf8]{inputenc}
%\usepackage [norsk]{babel} %if you write norwegian
\usepackage[english]{babel}  %if you write english


%% note that you may need to download some of these packages manually, it depends on your setup.
%% I recommend downloading TeXMaker, because it includes a large library of the most common packages.

\usepackage{physics,amssymb}  % mathematical symbols (physics imports amsmath)
\usepackage{graphicx}         % include graphics such as plots
\usepackage{xcolor}           % set colors
\usepackage{hyperref}         % automagic cross-referencing (this is GODLIKE)
\usepackage{tikz}             % draw figures manually
\usepackage{listings}         % display code
\usepackage{subfigure}        % imports a lot of cool and useful figure commands
\usepackage{verbatim}
\usepackage{adjustbox}


% defines the color of hyperref objects
% Blending two colors:  blue!80!black  =  80% blue and 20% black
\hypersetup{ % this is just my personal choice, feel free to change things
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}

%% Defines the style of the programming listing
%% This is actually my personal template, go ahead and change stuff if you want
\lstset{ %
	inputpath=,
	backgroundcolor=\color{white!88!black},
	basicstyle={\ttfamily\scriptsize},
	commentstyle=\color{magenta},
	language=Python,
	morekeywords={True,False},
	tabsize=4,
	stringstyle=\color{green!55!black},
	frame=single,
	keywordstyle=\color{blue},
	showstringspaces=false,
	columns=fullflexible,
	keepspaces=true}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\ihat}{\boldsymbol{\hat{\textbf{\i}}}}
\newcommand{\jhat}{\boldsymbol{\hat{\textbf{\j}}}}
\newcommand{\khat}{\boldsymbol{\hat{\textbf{k}}}}
\newcommand{\del}[1]{\textbf{#1)}}
\newcommand{\svar}[1]{\underline{\underline{{#1}}}}
\newcommand{\vc}[1]{\mathbf{#1}}

\graphicspath{{../output/}} % search for figures in this dir



\begin{document}


\begin{titlepage}
	\begin{center}
	\textbf{Diffusion Equation}

	\vspace{0.2cm}
	Vegard Falmår and Sigurd Sørlie Rustad

	\vspace{0.5cm}
	\includegraphics[scale=0.5]{../../pictures/UIO}
	\vspace{0.8cm}

	University of Oslo\\
	Norway\\
	\today	\\
	\end{center}
	\tableofcontents
	\clearpage
\end{titlepage}

\begin{abstract}
  Abstract
\end{abstract}
\maketitle                              % creates the title


\section{Introduction}

For our studies we have used c++ for heavy computation, python for visualization and automation. All the code along with instructions on how to run it, can be cloned from our GitHub repository\footnote{github.com/sigurdru/FYS3150/tree/master/Project5}.

\section{Theory}

\subsection*{The diffusion equation}

The full diffusion equation reads
\begin{equation*}
	\frac{\partial u(\mathbf{r}, t)}{\partial t} = \nabla \cdot \left[D(u, \mathbf{r})\nabla u(\mathbf{r}, t)\right],
\end{equation*}
with $\mathbf{r}$ is a positional vector. If $D(u,\mathbf{r}) = 1$ the equation simplifies to
\begin{equation*}
	\frac{\partial u}{\partial t} = \nabla^2u(\mathbf{r}, t),
\end{equation*}
or
\begin{equation}
	\label{eq:diffusion_equation}
	\left(\frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2}\right) u(x,y,z,t) = \frac{\partial u(x,y,z,t)}{\partial t}
\end{equation}
in cartesian coordinates. In this report we are mainly going to work with the diffusion equation in one and two dimensions, i.e.
\begin{equation*}
	\frac{\partial^2 u(x,t)}{\partial x^2} = \frac{\partial u(x,t)}{\partial t} \ \ \wedge \ \ \frac{\partial^2u(x,t)}{\partial x^2} + \frac{\partial^2u(x,t)}{\partial y^2} = \frac{\partial u(x,t)}{\partial t}.
\end{equation*}

\subsection*{Discretization}

Equation \eqref{eq:diffusion_equation} in one dimension reads
\begin{equation}
\frac{\partial^2u(x, t)}{\partial x^2} = \frac{\partial u(x,t)}{\partial t} \ \ \text{or} \ \ u_{xx} = u_t.
\end{equation}
With $x\in[0,L]$ and boundary conditions
\begin{equation}
\label{eq:boundary_conditions}
u(0, t) = a(t), \ \ t\geq 0 \ \ \wedge \ \ u(L,t) = b(t), \ \ t\geq 0,
\end{equation}
we can approximate the solution by discretization. First introducing $\Delta x = L/(n+1)$ and $\Delta t$ as small steps in $x$-direction and time. Then we can define the value domain of $t$ and $x$,
\begin{equation*}
t_j = j\Delta t, \ \ j\in \mathbb{N}_0 \ \ \wedge \ \ x_i = i\Delta x, \ \ \{i \in \mathbb{N}_0 | i \leq n + 1\}.
\end{equation*}
Where $n + 1$ is the number of points at which we evaluate $u(x,t)$.


\subsection*{Explicit and implicit schemes}

It is common divide numerical algorithms for integration into explicit and implicit schemes. When perfoming a numerical integration, we iterate over a discrete set of grid points at which we evaluate the function in question. In explicit schemes, the value at the next grid point is determined entirely by the value at the current grid point. In implicit schemes, the value at a later stage is determined by solving a system of equations containing both the current state and later states.

Using an implicit method instead of an explicit method usually requires more computation in every step, and they are often harder to implement. It can, in turn, save computation by allowing larger step sizes. Implicit methods can be (ARE?) unconditionally stable, meaning they are stable for any choice of step sizes. Explicit schemes are conditionally stable, meaning there is a relation between the step sizes in time and position that must be fulfilled in order to achieve stability. Of course, in order to achieve a desired  \textit{accuracy}, the step sizes can not be arbitrarily large for either the explicit or implicit schemes. It is generally the case, though, that implicit schemes allow for larger step sizes than explicit schemes.

REFERENCES, Intro to PDEs book, Wiki


\subsection*{Stability of explicit schemes}

The following stability requirement applies to problems of solving linear partial differential equations for both Dirichlet, Neumann, and periodic boundary conditions:
\begin{equation}
  \label{eq:von_neumann_stability}
  \frac{\Delta t}{(\Delta x)^2} \le 1/2
\end{equation}

REFERENCES, double check, book p 147, notes p 308


\subsection*{Explicit forward Euler}

The algorithm for explicit forward Euler in one dimension (from \cite{lectures2015} chapter 10.2.1) reads
\begin{equation}
  \label{eq:forward_euler}
	u_{i, j+1} = \alpha u_{i-1, j} + (1 - 2\alpha) u_{i,j} + \alpha u_{i+1, j}
\end{equation}
where
\begin{equation*}
	\alpha = \frac{\Delta t}{\Delta x^2},
\end{equation*}
and the discretization is explained in the appropriate section. Note that the expression on the right hand side, used to calculate the value at a time $t_j + \Delta t$, only contains the state of the system at time $t_j$.

%\begin{equation}
%	u_t \approx \frac{u(x, t+\Delta t) - u(x,t)}{\Delta t} = \frac{u(x_i,t_j+\Delta t) - u(x_i, t_j)}{\Delta t}
%\end{equation}
%and
%\begin{equation}
%	u_{xx} \approx \frac{u(x+\Delta x, t) - 2u(x,t) + u(x-\Delta x, t)}{\Delta x^2}
%\end{equation}
%or
%\begin{equation}
%	u_{xx} \approx \frac{u(x_i + \Delta x, t_j) - 2u(x_i,t_j) + u(x_i - \Delta x, t_j)}{\Delta x^2}
%\end{equation}
%and has a local approximation error $O(\Delta x^2)$. Symbols are defined in the discretization section. If we define $\alpha = \Delta t/\Delta x^2$ equ



\subsection*{Implicit Backward Euler}

The Backward Euler algorithm uses the same centered difference in space as Forward Euler to approximate the second derivative
\begin{equation}
  \label{eq:centered_2nd_deriv_space}
	u_{xx} \approx \frac{u(x_i + \Delta x, t_j) - 2u(x_i,t_j) + u(x_i-\Delta x, t_j))}{\Delta x^2},
\end{equation}
but a backward formula for the time derivative:
\begin{equation}
  \label{eq:backward_deriv_time}
  u_t \approx \frac{u(x_i, t_j) - u(x_i, t_j - \Delta t)}{\Delta t}
\end{equation}

Again, defining
\begin{equation*}
  \alpha = \frac{\Delta t}{(\Delta x)^2}
\end{equation*}
we obtain by inserting \eqref{eq:centered_2nd_deriv_space} and \eqref{eq:backward_deriv_time} into our differential equation the equation describing Backward Euler:
\begin{equation}
  \label{eq:backward_euler}
  u_{i, j-1} = -\alpha u_{i-1, j} + (1 + 2 \alpha) u_{i, j} - \alpha u_{i+1, j}
\end{equation}

Written out for all $i$, equation \ref{eq:backward_euler} becomes
\begin{align*}
  - \alpha u_{0, j} + (1 + 2 \alpha) u_{1, j} - \alpha u_{2, j} &= u_{1, j-1} \\
  - \alpha u_{1, j} + (1 + 2 \alpha) u_{2, j} - \alpha u_{3, j} &= u_{2, j-1} \\
  ... \\
  - \alpha u_{n-3, j} + (1 + 2 \alpha) u_{n-2, j} - \alpha u_{n-1, j} &= u_{n-2, j-1} \\
  - \alpha u_{n-2, j} + (1 + 2 \alpha) u_{n-1, j} - \alpha u_{n, j} &= u_{n-1, j-1} \\
\end{align*}
In general, this can be rearranged slightly so that
\begin{align*}
  (1 + 2 \alpha) u_{1, j} -\alpha  u_{2, j} &= u_{1, j-1} + \alpha u_{0, j} \\
  - \alpha u_{1, j} + (1 + 2 \alpha) u_{2, j} - \alpha u_{3, j} &= u_{2, j-1} \\
  ... \\
  - \alpha u_{n-3, j} + (1 + 2 \alpha) u_{n-2, j} - \alpha u_{n-1, j} &= u_{n-2, j-1} \\
  - \alpha u_{n-2, j} + (1 + 2 \alpha) u_{n-1, j} &= u_{n-1, j-1} + \alpha u_{n, j} \\
\end{align*}
Let $\vc v_j$ be a vector containing the values of $u$ at $n - 1$ points in space at a time $t_j$
\begin{equation}
  \label{def:vector_v}
  \vc v_j =
  \begin{bmatrix}
    u_{1, j} \\
    u_{2, j} \\
    \vdots \\
    u_{n-2, j} \\
    u_{n-1, j} \\
  \end{bmatrix}
\end{equation}
and the vector $\vc b_j$ be defined as follows:
\begin{equation}
  \label{def:vector_b_BackwardEuler}
  \vc b_j =
  \begin{bmatrix}
    u_{1, j-1} + \alpha u_{0, j} \\
    u_{2, j-1} \\
    \vdots \\
    u_{n-2, j-1} \\
    u_{n-1, j-1} + \alpha u_{n, j} \\
  \end{bmatrix}
\end{equation}
As the boundary conditions $u_{0, j}$ and $u_{n, j}$ are specified, we already know every component of $\vc b_j$. We can then rewrite equation \eqref{eq:backward_euler} as the matrix equation
\begin{equation}
  \label{eq:backward_euler_matrix}
  \vc A \vc v_j = \vc b_j
\end{equation}
where $\vc A$ is defined as
\begin{equation}
  \label{def:matrix_A}
  A =
  \begin{bmatrix}
    (1 + 2 \alpha) & -\alpha         & 0       & 0      & \ldots         & 0 \\
    -\alpha        & (1 + 2 \alpha)  & -\alpha & 0      & \ldots         & 0 \\
    \vdots         &                 & \ddots  &        &                & \vdots \\
    0              & \ldots          & 0       &-\alpha & (1 + 2 \alpha) & -\alpha  \\
    0              & \ldots          & 0       & 0      &-\alpha         & (1 + 2 \alpha)  \\
  \end{bmatrix}
\end{equation}
This is a tridiagonal matrix with $(1 + 2 \alpha)$ on the diagonal and $-\alpha$ directly above and below the diagonal.

MOVE SOMETHING TO METHODS?


\subsection*{Implicit Crank-Nicolson}

The Crank-Nicolson algorithm uses a time-centered scheme centered around $t + \Delta t/2$. The time derivative is given by
\begin{equation}
  \label{eq:forward_deriv_time}
  u_t \approx \frac{u(x_i, t_j + \Delta t) - u(x_i, t_j)}{\Delta t}
\end{equation}
\begin{align*}
	u_{xx} &\approx \frac{1}{2 \Delta x^2} \bigg(u(x_i+\Delta x, t_j) - 2u(x_i,t_j) + u(x_i-\Delta x, t_j) \\
  	&\quad + u(x_i+\Delta x, t_j + \Delta t) - 2u(x_i, t_j + \Delta t) \\
  	&\quad + u(x_i - \Delta x, t_j + \Delta t) \bigg)
\end{align*}

Inserting these two equations into our differential equation, we obtain (see Appendix, section \ref{sect:Crank-Nicolson_derivation} for derivation) that the equation describing the Crank-Nicolson algorithm can be written as a matrix equation
\begin{equation}
  \label{eq:Crank-Nicolson_matrix}
  \vc A \vc v_j = \vc b_j
\end{equation}
The vector $\vc v_j$ is the same as in the case of Backward Euler, defined in \eqref{def:vector_v}. The matrix $\vc A$ is a tridiagonal matrix with $2 (1 + \alpha)$ on the diagonal and $-\alpha$ directly above and below the diagonal. The vector $\vc b_j$ is
\begin{equation}
  \label{def:vector_b_CrankNicolson}
  \vc b_j =
  \begin{bmatrix}
    \alpha u_{0, j}  + 2 (1 - \alpha) u_{1,j} + \alpha u_{2, j} + \alpha u_{0, j+1} \\
    \alpha u_{1, j}  + 2 (1 - \alpha) u_{2,j} + \alpha u_{3, j} \\
      \vdots \\
    \alpha u_{n-3, j}  + 2 (1 - \alpha) u_{n-2,j} + \alpha u_{n-1, j} \\
    \alpha u_{n-2, j}  + 2 (1 - \alpha) u_{n-1,j} + \alpha u_{n, j} + \alpha u_{n, j+1}
  \end{bmatrix}
\end{equation}



\section{Methods}

We will start by solving the one dimensional diffusion equation
\begin{equation*}
  \frac{\partial^2 u(x,t)}{\partial x^2} = \frac{\partial u(x,t)}{\partial t}, \quad x \in [0, L]
\end{equation*}
with initial conditions
\begin{equation*}
  u(x, 0) = 0
\end{equation*}
and boundary conditions
\begin{align*}
  u(0, t) &= 0, \quad t \ge 0 \quad \text{and} \\
  u(L, t) &= 1, \quad t \ge 0
\end{align*}


\subsection*{Solving the tridiagonal matrix systems}

Solving equations \eqref{eq:backward_euler_matrix} and REF is done by inverting the matrix $\vc A$. Since this matrix never changes, invert once instead of every time step??

Specialized algorithms can be developed for solving tridiagonal matrix systems. We will be reusing code developed for a previous project\footnote{https://github.com/sigurdru/FYS3150/tree/master/Project1}.



\section{Results}

Results

\section{Discussion}

Discussion

\section{Conclusion}

Conclusion


\onecolumngrid
\vspace{1cm} % some extra space

\begin{thebibliography}{}
\bibitem[]{lectures2015} Morten Hjorth-Jensen, Computational Physics, Lecture Notes Fall 2015, August 2015, https://github.com/CompPhysics/ComputationalPhysics/blob/master/doc/Lectures/lectures2015.pdf.

\end{thebibliography}


\section{Appendix}

\subsection{Derivation of equation for the Crank-Nicolson scheme} \label{sect:Crank-Nicolson_derivation}

Again we define
\begin{equation*}
  \alpha = \frac{\Delta t}{(\Delta x)^2}
\end{equation*}
Inserting the approximations of $u_t$ and $u_{xx}$ stated in the Theory section into our differential equation, we get
\begin{align*}
  u_t &= u_{xx} \\
  \frac{u_{i, j+1} - u_{i, j}}{\Delta t}
    &= \frac{1}{2 \Delta x^2} \bigg(u_{i+1, j} - 2u_{i,j} + u_{i-1, j}
      + u_{i+1, j + 1} - 2u_{i, j + 1} + u_{i - 1, j + 1} \bigg) \\
  u_{i, j+1} - u_{i, j} &= \frac{\alpha}{2} \bigg(u_{i+1, j} - 2u_{i,j} + u_{i-1, j}
      + u_{i+1, j+1} - 2u_{i, j+1} + u_{i-1, j+1} \bigg) \\
  u_{i, j+1} - \frac{\alpha}{2} \bigg( u_{i+1, j+1} - 2u_{i, j+1} + u_{i-1, j+1} \bigg)
    &= u_{i, j} + \frac{\alpha}{2} \bigg(u_{i+1, j}  - 2u_{i,j} + u_{i-1, j} \bigg) \\
  2 u_{i, j+1} - \alpha u_{i+1, j+1} + 2 \alpha u_{i, j+1} - \alpha u_{i-1, j+1}
    &= 2 u_{i, j} + \alpha u_{i+1, j}  - 2 \alpha u_{i,j} + \alpha u_{i-1, j} \\
  - \alpha u_{i-1, j+1} + 2 (1 + \alpha) u_{i, j+1} - \alpha u_{i+1, j+1}
    &= \alpha u_{i-1, j}  + 2 (1 - \alpha) u_{i,j} + \alpha u_{i+1, j}
\end{align*}
Written out explicitly for all $i$, the equation reads
\begin{align*}
  - \alpha u_{0, j+1} + 2 (1 + \alpha) u_{1, j+1} - \alpha u_{2, j+1}
    &= \alpha u_{0, j}  + 2 (1 - \alpha) u_{1,j} + \alpha u_{2, j} \\
  - \alpha u_{1, j+1} + 2 (1 + \alpha) u_{2, j+1} - \alpha u_{3, j+1}
    &= \alpha u_{1, j}  + 2 (1 - \alpha) u_{2,j} + \alpha u_{3, j} \\
    \vdots \\
  - \alpha u_{n-3, j+1} + 2 (1 + \alpha) u_{n-2, j+1} - \alpha u_{n-1, j+1}
    &= \alpha u_{n-3, j}  + 2 (1 - \alpha) u_{n-2,j} + \alpha u_{n-1, j} \\
  - \alpha u_{n-2, j+1} + 2 (1 + \alpha) u_{n-1, j+1} - \alpha u_{n, j+1}
    &= \alpha u_{n-2, j}  + 2 (1 - \alpha) u_{n-1,j} + \alpha u_{n, j}
\end{align*}

Similar to the case of Backward Euler, this set of equations can be rearranged as follows
\begin{align*}
  2 (1 + \alpha) u_{1, j+1} - \alpha u_{2, j+1}
    &= \alpha u_{0, j}  + 2 (1 - \alpha) u_{1,j} + \alpha u_{2, j} + \alpha u_{0, j+1} \\
  - \alpha u_{1, j+1} + 2 (1 + \alpha) u_{2, j+1} - \alpha u_{3, j+1}
    &= \alpha u_{1, j}  + 2 (1 - \alpha) u_{2,j} + \alpha u_{3, j} \\
    \vdots \\
  - \alpha u_{n-3, j+1} + 2 (1 + \alpha) u_{n-2, j+1} - \alpha u_{n-1, j+1}
    &= \alpha u_{n-3, j}  + 2 (1 - \alpha) u_{n-2,j} + \alpha u_{n-1, j} \\
  - \alpha u_{n-2, j+1} + 2 (1 + \alpha) u_{n-1, j+1}
    &= \alpha u_{n-2, j}  + 2 (1 - \alpha) u_{n-1,j} + \alpha u_{n, j} + \alpha u_{n, j+1}
\end{align*}
We can define the vector $\vc b_j$ to hold the values on the right side of each of these equations:
\begin{equation}
  \label{def:vector_b_CrankNicolson}
  \vc b_j =
  \begin{bmatrix}
    \alpha u_{0, j}  + 2 (1 - \alpha) u_{1,j} + \alpha u_{2, j} + \alpha u_{0, j+1} \\
    \alpha u_{1, j}  + 2 (1 - \alpha) u_{2,j} + \alpha u_{3, j} \\
      \vdots \\
    \alpha u_{n-3, j}  + 2 (1 - \alpha) u_{n-2,j} + \alpha u_{n-1, j} \\
    \alpha u_{n-2, j}  + 2 (1 - \alpha) u_{n-1,j} + \alpha u_{n, j} + \alpha u_{n, j+1}
  \end{bmatrix}
\end{equation}
Using this and the definition of the vector $\vc v_j$ from \eqref{def:vector_v}, we can write the set of equations as
\begin{equation*}
  \vc A \vc v_j = \vc b_j
\end{equation*}
where the matrix $\vc A$ is a tridiagonal matrix with $2 (1 + \alpha)$ on the diagonal and $-\alpha$ directly above and below the diagonal.


\end{document}
