\documentclass[reprint, english,notitlepage]{revtex4-1}  % defines the basic parameters of the document
% if you want a single-column, remove reprint

% allows special characters (including æøå)
\usepackage[utf8]{inputenc}
% \usepackage [norsk]{babel} %if you write norwegian
\usepackage[english]{babel}  %if you write english


%% note that you may need to download some of these packages manually, it depends on your setup.
%% I recommend downloading TeXMaker, because it includes a large library of the most common packages.

\usepackage{physics,amssymb}  % mathematical symbols (physics imports amsmath)
\usepackage{graphicx}         % include graphics such as plots
\usepackage{xcolor}           % set colors
\usepackage{hyperref}         % automagic cross-referencing (this is GODLIKE)
\usepackage{tikz}             % draw figures manually
\usepackage{listings}         % display code
\usepackage{subfigure}        % imports a lot of cool and useful figure commands
\usepackage{verbatim}
\usepackage{adjustbox}


% defines the color of hyperref objects
% Blending two colors:  blue!80!black  =  80% blue and 20% black
\hypersetup{ % this is just my personal choice, feel free to change things
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}

%% Defines the style of the programming listing
%% This is actually my personal template, go ahead and change stuff if you want
\lstset{ %
	inputpath=,
	backgroundcolor=\color{white!88!black},
	basicstyle={\ttfamily\scriptsize},
	commentstyle=\color{magenta},
	language=Python,
	morekeywords={True,False},
	tabsize=4,
	stringstyle=\color{green!55!black},
	frame=single,
	keywordstyle=\color{blue},
	showstringspaces=false,
	columns=fullflexible,
	keepspaces=true}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\ihat}{\boldsymbol{\hat{\textbf{\i}}}}
\newcommand{\jhat}{\boldsymbol{\hat{\textbf{\j}}}}
\newcommand{\khat}{\boldsymbol{\hat{\textbf{k}}}}
\newcommand{\del}[1]{\textbf{#1)}}
\newcommand{\svar}[1]{\underline{\underline{{#1}}}}


\begin{document}



\title{FYS3150 - Project 1}
\date{\today}
\author{Sigurd Sørlie Rustad and Vegard Falmår}


\newpage

\begin{abstract}
We have explored three different numerical implementations of a finite difference method for solving the one-dimensional Poisson equation. This is a second order partial differential equation. The main goal of the project was to observe how the relative error relates to the step size of discretization. As expected, we saw a higher degree of precision with smaller step sizes, until loss of numerical precision started to affect the results causing the relative error to increase. We also derived how the number of FLOPs, and thus the time needed for computation, depend on the number of steps for the different algorithms. We were then able to briefly discuss the tradeoff between high precision, low computational time and memory usage.

By discretizing the equation, one can rewrite it to a system of linear equations, in truth a quite simple linear algebra problem. The three different methods we have implemented for solving this set of linear equations is the general LU decomposition of a matrix, a general method for solving linear equations with tridiagonal matrices and finally a highly specialized algorithm for the specific matrix that arises from discretizing the second derivative of a function.

We obtained the best results with the specialized algorithm. With step size $h = 10^{-6}$ we got results with a maximum relative error of as little as $10^{-10}$. Beyond that, loss of numerical precision gave reduced precision. For the general tridiagonal algorithm, we observed a serious loss of precision at $h = 10^{-6}$. Both $h = 10^{-5}$ and $h = 10^{-4}$ gave better presicion than $h = 10^{-6}$. The LU decomposition is unable to handle step sizes smaller than $10^{-4}$ due to memory comsumption.
\end{abstract}
\maketitle                                % creates the title, author, date & abstract


\section{Introduction}

We can describe the evolution of many physical systems with the help of differential equations, but because of their complexity we are often unable to find analytical solutions. Therefore we have to use numerical methods in order to approximate the solution. Computers are limited in both their memory and accuracy, so we have to be careful when both selecting the numerical method and how we implement it. In this report we are going to explore these issues by trying to solve the one-dimensional Poisson equation with Dirichlet boundary conditions, described in equation \ref{eq:Poisson_1D} in the theory section.

The example function we are going to use in our studies is described by the equation (\ref{eq:f}) and has an analytical solution (\ref{eq:u}) we can use to compare our results.
\begin{equation}
	-\frac{d^2u}{dx^2} = f(x) = 100e^{-10x}
	\label{eq:f}
\end{equation}
\begin{equation}
	u(x) = 1 - (1 - e^{-10})x - e^{-10x}
	\label{eq:u}
\end{equation}
In order to solve equation (\ref{eq:f}) we end up with a set of linear equations described by a tridiagonal matrix multiplied with a vector (see the theory section for further explanation). Now there are many ways we can solve this set of equations, each with their own pros and cons. The methods we are going to explore is one where we solve for a general tridiagonal matrix, one where we specialize the algorithm to our tridiagonal matrix and lastly by using LU decomposition. For each method we will study the accuracy, cpu-time used and the number of floating-point operations (FLOPS).

For our studies we have used c++ for heavy computation, python for visualization and bash for automation. All the code along with instructions on how to run it, can be cloned fro our GitHub repository here \citep{github}.



\section{Theory}


\subsection{Matrix formulation of the discrete one-dimensional Poisson equation}
The one-dimensional Poisson equation with Dirichlet boundary conditions can be written as equation \ref{eq:Poisson_1D}.
\begin{equation}
  \label{eq:Poisson_1D}
  - \frac{\mathrm d^2 u(x)}{\mathrm d x^2} = f(x), \quad x \in (0, 1), \quad u(0) = u(1) = 0
\end{equation}
We define the discretized approximation of $u$ to be $v_i$ at points $x_i = i h$ evenly spaced between $x_0 = 0$ and $x_{n - 1} = 1$. The step length between the points is $h = 1/(n - 1)$. The boundary conditions from equation \ref{eq:Poisson_1D} then give $v_0 = v_{n - 1} = 0$. Deriving an approximation to the second derivative of $u$ from the Taylor expansion, equation \ref{eq:Poisson_1D} can be written
\begin{equation}
  \label{eq:discrete_2nd_derivative}
  \frac{-v_{i-1} + 2 v_i - v_{i+1}}{h^2} = f_i \quad \text{for } i = 1, 2, ..., n-2
\end{equation}
where $f_i = f(x_i)$.

Written out for all $i$, equation \ref{eq:discrete_2nd_derivative} becomes

\begin{align*}
  - v_0 + 2 v_1 - v_2 &= h^2 f_1 \\
  - v_1 + 2 v_2 - v_3 &= h^2 f_2 \\
  ... \\
  - v_{n-4} + 2 v_{n-3} - v_{n-2} &= h^2 f_{n-3} \\
  - v_{n-3} + 2 v_{n-2} - v_{n-1} &= h^2 f_{n-2} \\
\end{align*}
In general, this can be rearranged slightly so that
\begin{align*}
  2 v_1 - v_2 &= h^2 f_1 + v_0\\
  - v_1 + 2 v_2 - v_3 &= h^2 f_2 \\
  ... \\
  - v_{n-4} + 2 v_{n-3} - v_{n-2} &= h^2 f_{n-3} \\
  - v_{n-3} + 2 v_{n-2} &= h^2 f_{n-2} + v_{n-1} \\
\end{align*}
This system of equations can be written in matrix form as
\begin{equation}
  \label{eq:Poisson_1D_matrix}
  \boldsymbol A \boldsymbol v = \boldsymbol{\tilde{b}},
\end{equation}
explicitly
\begin{equation*}{}
  \begin{bmatrix}
2  & -1 & 0  & 0 & ... & 0 \\
-1 & 2  & -1 & 0 & ... & 0 \\
.  &    &    &   &     & \\
.  &    &    &   &     & \\
.  &    &    &   &     & \\
0  & ...& 0  &-1 & 2   & -1  \\
0  & ...& 0  & 0 &-1 & 2  \\
\end{bmatrix}
  \begin{bmatrix}
 v_1  \\
 v_2  \\
.   \\
.   \\
.   \\
 v_{n-3} \\
v_{n-2} \\
\end{bmatrix}
=
\begin{bmatrix}
h^2 f_1 + v_0 \\
h^2 f_2  \\
.   \\
.   \\
.   \\
h^2 f_{n-3} \\
h^2 f_{n-2} + v_{n-1}\\
\end{bmatrix}
\end{equation*}
With $v_0 = v_{n-1} = 0$, the right side reduces to $\tilde b_i = h^2 f_i$ for $i = 1, 2, ..., n-2$.


\subsection{Lower-upper (LU) decomposition}

LU decomposition is a method where you factorize a matrix $A$ into two matrices $L$ and $U$, where $L$ is lower triangular and $U$ is upper triangular. We can use this decomposition to solve a matrix equation.
\begin{equation*}
	A\mathbf{x} = LU\mathbf{x} = \mathbf{b}
\end{equation*}
Where $A$ and $\mathbf{b}$ is known. First we can solve $L\mathbf{y}=\mathbf{b}$ with the algorithm given by equation (\ref{eq:lu1}) (see \citep{lu}).
\begin{equation}
	y_i = \frac{1}{l_{ii}}\bigg(b_i - \sum_{j=1}^{i-1}l_{ij}y_j\bigg),\ \ y_1 = \frac{b_1}{l_{11}}
	\label{eq:lu1}
\end{equation}
Where $y_i$ is element $i$ in $\mathbf{y}$, $l_{ij}$ element $ij$ in matrix $L$ and $b_i$ element $i$ in $\mathbf{b}$. We can then solve $U\mathbf{x} = \mathbf{y}$ with the algorithm from equation (\ref{eq:lu2}) (see \citep{lu}).
\begin{equation}
	x_i = \frac{1}{u_{ii}}\bigg(y_i - \sum_{j=i+1}^{N}u_{ij}x_j\bigg),\ \ x_N=\frac{y_N}{u_{NN}}
	\label{eq:lu2}
\end{equation}
Here $x_i$ is element $i$ in $\mathbf{x}$.


\subsection{Floating-point operations}
Whenever you do a mathematical operation on a computer you call it a floating-point operation. This includes division, multiplication, subtraction and addition. Floating-point operations (what we from now on will refer to as FLOPS) is a count of the total number of mathematical operations needed for an algorithm.

Many FLOPS usually leads to slower code, and if you have round off errors it can propagate and create larger errors than expected.


\subsection{Relative error}
To better interpret results it can be a good idea to calculate the relative error. The formula for calculating the relative error is given by equation (\ref{eq:rel_err}).
\begin{equation}
	E = \frac{x_0 - x}{x}
	\label{eq:rel_err}
\end{equation}
Where $E$ is the relative error, $x_0$ the computed result and $x$ the actual result. In this project, we will treat the logarithm of the relative error, given by equation (\ref{eq:log_err}).
\begin{equation}
	\epsilon = log_{10}(\abs{E}) = log_{10}\bigg(\abs{\frac{x_0-x}{x}}\bigg)
	\label{eq:log_err}
\end{equation}



\section{Method}



\subsection{Solve tridiagonal matrix equation}

In order to solve the tridiagonal matrix below we need to develop an algorithm. As mentioned in the exercise set \citep{oppgavetekst} we first need to do a decomposition and forward substitution.
\begin{equation}
\mathbf{A}\mathbf{v} = \begin{bmatrix}
b_1& c_1 & 0 &\dots   & \dots &\dots \\
a_1 & b_2 & c_2 &\dots &\dots &\dots \\
& a_2 & b_3 & c_3 & \dots & \dots \\
& \dots   & \dots &\dots   &\dots & \dots \\
&   &  &a_{n-2}  &b_{n-1}& c_{n-1} \\
&    &  &   &a_{n-1} & b_n \\
\end{bmatrix}\begin{bmatrix}
v_1\\
v_2\\
\dots \\
\dots  \\
\dots \\
v_n\\
\end{bmatrix}
=\begin{bmatrix}
\tilde{b}_1\\
\tilde{b}_2\\
\dots \\
\dots \\
\dots \\
\tilde{b}_n\\
\end{bmatrix}
\label{eq:mat_gen}
\end{equation}

Looking at the first matrix multiplication we get the following expression.

\begin{equation}
	b_1 v_1 + c_1 v_2 = \tilde{b} \implies v_1 + \alpha_1 v_2 = \rho _1, \ \ \alpha_1 = \frac{c_1}{b_1} \wedge \rho_1 = \frac{\tilde{b}_1}{b_1}
	\label{eq:mat1}
\end{equation}

Doing the second matrix multiplication we get

\begin{equation}
	a_1 v_1 + b_2 v_2 + c_2 v_3 = \tilde{b}_2
	\label{eq:mat2}
\end{equation}

If we multiply equation \ref{eq:mat1} by $a_1$, and subtract it from equation \ref{eq:mat2} the resulting expression becomes

\begin{align*}
	(b_2 - \alpha_1 a_1) v_2 + c_2 v_3 &= \tilde{b}_2 - \rho_1 a_1 \\
	\implies v_2 + \frac{c_2}{b_2 - \alpha_1 a_1}v_3 &= \frac{\tilde{b}_2 - \rho_1 a_1}{b_2 - \alpha_1 a_1} \\
	\implies v_2 + \alpha_2 v_3 &= \rho_2 \\
	\text{where}\ \ \alpha_2 = \frac{c_2}{b_2 - \alpha_1 a_1} &\wedge \rho_2 = \frac{\tilde{b}_2 - \rho_1 a_1}{b_2 - \alpha_1 a_1}
\end{align*}

Noticing the pattern in $\rho$ and $\alpha$ we can generalize the terms.

\begin{equation}
	\alpha_i = \frac{c_i}{b_i - \alpha_{i-1}a_{i-1}} \ \ \text{for} \ \ i = 2, 3, ..., n-1
	\label{eq:alpha}
\end{equation}
\begin{equation}
	\rho_i = \frac{\tilde{b}_{i} - \rho_{i-1} a_{i-1}}{b_{i} - \alpha_{i-1} a_{i-1}}\ \ \text{for} \ \ i = 2, 3, ..., n
	\label{eq:rho}
\end{equation}

Inserting the terms into the matrix above, we get a much simpler set of equations.

\begin{equation*}
	\mathbf{A}\mathbf{v} = \begin{bmatrix}
		1& \alpha_1 & 0 &\dots   & \dots &\dots \\
		0 & 1 & \alpha_2 &\dots &\dots &\dots \\
		& 0 & 1 & \alpha_3 & \dots & \dots \\
		& \dots   & \dots &\dots   &\dots & \dots \\
		&   &  &0  &1& \alpha_{n-1} \\
		&    &  &   &0 & 1 \\
	\end{bmatrix}\begin{bmatrix}
		v_1\\
		v_2\\
		\dots \\
		\dots  \\
		\dots \\
		v_n\\
	\end{bmatrix}
	=\begin{bmatrix}
		\rho_1\\
		\rho_2\\
		\dots \\
		\dots \\
		\dots \\
		\rho_n\\
	\end{bmatrix}.
\end{equation*}

Now the last step is to do a backward substitution. Starting with $v_n = \rho_n$ we can work our way backward, with the general expression
\begin{equation}
	v_{i-1} = \rho_{i-1} - \alpha_{i-1}v_i \ \ \text{for} \ \ i = n, n-1, ..., 2
	\label{eq:v}
\end{equation}

Now in this report we are going to consider a matrix with elements $b_n = 2$ and $a_n = c_n = -1$. We can insert this into equations (\ref{eq:alpha}) and (\ref{eq:rho}) to get the expressions (\ref{eq:alpha_spes}) and (\ref{eq:rho_spes}).
\begin{equation}
\alpha_i = \frac{-1}{2 + \alpha_{i-1}}
\label{eq:alpha_spes}
\end{equation}
\begin{equation}
	\rho_i = \frac{\tilde{b} + \rho_{i-1}}{2 + \alpha_{i-1}}
	\label{eq:rho_spes}
\end{equation}


\subsection{Specialized algorithm for our spesific problem}

Our specific problem is defined by the matrix equation \ref{eq:Poisson_1D_matrix}. We will stay with only the left side of the equation to save som space during the derivation. In order to solve this matrix equation, we want to perform row operations on the matrix $\boldsymbol A$ to transform it to the identity matrix. We start by dividing the first line by 2, giving
\begin{equation*}{}
  \begin{bmatrix}
1  & -1/2 & 0  & 0 & ... & 0 \\
-1 & 2  & -1 & 0 & ... & 0 \\
.  &    &    &   &     & \\
.  &    &    &   &     & \\
.  &    &    &   &     & \\
0  & ...& 0  &-1 & 2   & -1  \\
0  & ...& 0  & 0 &-1 & 2  \\
\end{bmatrix}
\end{equation*}
Now, adding the first line to the second gives
\begin{equation*}{}
  \begin{bmatrix}
1  & -1/2 & 0  & 0  & ... & 0 \\
0  & 3/2  & -1 & 0  & ... & 0 \\
0  & -1   &  2 & -1 &     & \\
.  &    &    &   &     & \\
.  &    &    &   &     & \\
.  &    &    &   &     & \\
0  & ...& 0  &-1 & 2   & -1  \\
0  & ...& 0  & 0 &-1 & 2  \\
\end{bmatrix}
\end{equation*}
Multiplying line 2 with $2/3$ gives
\begin{equation*}{}
  \begin{bmatrix}
1  & -1/2 & 0  & 0  & ... & 0 \\
0  & 1  & -2/3 & 0  & ... & 0 \\
0  & -1   &  2 & -1 &     & \\
.  &    &    &   &     & \\
.  &    &    &   &     & \\
.  &    &    &   &     & \\
0  & ...& 0  &-1 & 2   & -1  \\
0  & ...& 0  & 0 &-1 & 2  \\
\end{bmatrix}
\end{equation*}
Already, we see a pattern. On each line from line 2 onwards, we will add the previous line, giving us 0 to the left of the diagonal. It looks like the element to the right of the diagonal on line $i-1$ is given by
\begin{equation}
  \label{eq:right_of_diag_specialized_algo}
  A_{i-1, i} = -\frac{i-1}{i}
\end{equation}
As we add line $i-1$ to line $i$, the diagonal element of line $i$ becomes
\begin{equation}
  \label{eq:diagonal_element_specialized_algo}
  2 - \frac{i-1}{i} = \frac{2i - i + 1}{i} = \frac{i+1}{i}.
\end{equation}
We will then multiply line $i$ with $\frac{i}{i+1}$ giving us 1 on the diagonal and $-\frac{i}{i+1}$ directly to the right of the diagonal. This is the exact expression we assumed in expression \ref{eq:right_of_diag_specialized_algo}, showing that it is in fact correct.

The forward substitution can therefore be done by dividing the first line by 2, and then for each following line (numbered by $i$), add the previous line and multiply the result with $\frac{i}{i+1}$. After this procedure we end up with the following matrix
\begin{equation*}{}
  \begin{bmatrix}
1  & -1/2 & 0  & 0  & ... & 0 \\
0  & 1  & -2/3 & 0  & ... & 0 \\
0  & 0  &  1 & -3/4 &     & \\
.  &    &    &   &     & \\
.  &    &    &   &     & \\
.  &    &    &   &     & \\
0  & ...& 0  & 0 & 1 & -(n - 3)/(n - 2)  \\
0  & ...& 0  & 0 & 0 & 1  \\
\end{bmatrix}
\end{equation*}

The backward substitution can then be done with a simple expression. Starting from the second to last line, numbered by $i = n-3$, we would like to add $\frac{i}{i+1}$ times the line below.

The forward and backward substitution can be performed by the following lines of code. Assume we have already filled the array $\boldsymbol{v}$ with values $v_i = h^2 f_i$ for $i = 1, 2, ..., n-2$.
\begin{verbatim}
  v[0] = v[n-1] = 0   # Boundary conditions
  v[1] = v[1]/2   # Divide the first line by 2

  for (i = 2; i < n-1; i++) {
      v[i] = i/(i+1) * (v[i] + v[i-1])
  }
  for (i = n-3; i > 0; i--) {
      v[i] = v[i] + i/(i+1) * v[i+1]
  }
\end{verbatim}

By storing the values $i/(i+1)$ in an array, we can replace these two FLOPS with a memory fetch in our loops. We then have 2 FLOPS per iteration in the forward substitution (one addition and one multiplication) and two FLOPS per iteration in the backward substitution (one addition and one multiplication). For large values of $n$, the total number of FLOPS for the specialized algorithm is then
\begin{equation}
  \label{eq:flops_specialized}
  N_{\text{fast}} \approx 4n
\end{equation}

First we implement the numerical methods discussed in the theory section. As mentioned in the introduction, all the code and how to run it can be found in our GitHub repository here \citep{github}. To implement the general algorithm, where we don't assume identical elements in the tridiagonal matrix (see equation (\ref{eq:mat_gen})), we use the algorithms described in equations (\ref{eq:alpha}), (\ref{eq:rho}) and (\ref{eq:v}). Counting the number of FLOPS in the algorithm we notice $N_1 = 3(n-2)$ from equation (\ref{eq:alpha}) (3 FLOPS per iteration, $n-2$ iterations), $N_2 = 5(n-1)$ from equation (\ref{eq:rho}) (5 FLOPS per iteration $n-1$ iterations) and $N_3 = 3(n-2)$ from equation (\ref{eq:v}) (3 FLOPS per iteration $n-1$ iterations). Now to find the total number of flops for our algorithm we can sum them up.
\begin{equation*}
	N_{tot} = N_1+N_2+N_3 = 5(n-1) + 6(n-2) \approx 11n
\end{equation*}
The approximation holds when $n$ is large. Running the algorithm for $n=10^i,\ \ i = 2, 3, ..., 6$, we can plot the result, time the code and find the relative error using equation (\ref{eq:log_err}).

In order to implement the LU decomposition we use the armadillo library (see \citep{armadillo} for documentation). From \citep{lu_wiki} we get that the LU decomposition itself requires $N_{LU} = 2n^3/3$ FLOPS. Looking at the equations (\ref{eq:lu1}) and (\ref{eq:lu2}) we can count the rest of the FLOPS. From equation (\ref{eq:lu1}) we get approximately $N \approx 4n$, giving us a total of $N_{tot} = 2n^3/3 + 4n$ FLOPS. Again running the algorithm for $n=10^i,\ \ i = 2, 3, 4, 5$, we can plot the result, time the code and find the relative error using equation (\ref{eq:log_err}).


\subsection{Code tests}

We have constructed a quite simple test to assure that our algorithms performs as expected. Our code is written to solve the matrix equation
\begin{equation*}{}
  \begin{bmatrix}
2  & -1 & 0  & 0 & ... & 0 \\
-1 & 2  & -1 & 0 & ... & 0 \\
.  &    &    &   &     & \\
.  &    &    &   &     & \\
.  &    &    &   &     & \\
0  & ...& 0  &-1 & 2   & -1  \\
0  & ...& 0  & 0 &-1 & 2  \\
\end{bmatrix}
  \begin{bmatrix}
 v_1  \\
 v_2  \\
.   \\
.   \\
.   \\
 v_{n-3} \\
v_{n-2} \\
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\
b_2 \\
.   \\
.   \\
.   \\
b_{n-3} \\
b_{n-2} \\
\end{bmatrix}
\end{equation*}
with $v_0 = v_{n-1} = 0$. We have used the following analytic solution to construct our test
\begin{equation*}
\begin{bmatrix}
b_1 \\
b_2 \\
b_3 \\
b_4 \\
b_5 \\
\end{bmatrix}
 =
 \begin{bmatrix}
 1 \\
 2 \\
 3   \\
 4   \\
 6   \\
 \end{bmatrix}
 \implies
 \begin{bmatrix}
 v_1 \\
 v_2 \\
 v_3 \\
 v_4 \\
 v_5 \\
 \end{bmatrix}
  =
  \begin{bmatrix}
  6 \\
  11 \\
  14  \\
  14  \\
  10  \\
  \end{bmatrix}
\end{equation*}
By providing this $\boldsymbol b$ as input and verifying that the computed solution is
\begin{equation*}
  \boldsymbol v
=
\begin{bmatrix}
0 \\
6 \\
11 \\
14  \\
14  \\
10  \\
0 \\
\end{bmatrix}
\end{equation*}
we have checked that our code solves the matrix equation correctly.



\section{Results}

Figure \ref{fig:slow_2} show the results obtained with step size $h = 10^{-2}$. The computed solution is plotted with the exact analytic solution. The plots for all the other step sizes can be found in the project GitHub repository \citep{github} or can be produced as explained in the README-file of the project. They are all, however, indistinguishable from one another.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.44]{../output/slow_2.pdf}
	\label{fig:slow_2}
	\caption{This figure shows the numeric solution for our general algorithm and the exact solution on top (green dashed line). The step size used is $h=10^{-2}$.}
\end{figure}

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[scale=0.44]{../output/slow_6.pdf}
% 	\label{fig:slow_6}
% 	\caption{This figure shows the numeric solution for our general algorithm and the exact solution on top (green dashed line). The step size used is $h=10^{-6}$.}
% \end{figure}

Tables \ref{tab:slow_error}, \ref{tab:fast_error} and \ref{tab:lu_error} and figures \ref{fig:slow_error}, \ref{fig:fast_error} and \ref{fig:lu_error} show the values of $\epsilon$ (errors) for the different algorithms for all steps sizes.

\begin{table} [h]  %[p] % Uncomment to put table in Appendix
  \input{../output/slow_errors_tex.txt}
	\caption{This table shows the relative error for our general method. The firs column is the logarithm of our step size and the second column is the relative error (see figure \ref{fig:slow_error} for table plotted).}
	\label{tab:slow_error}
\end{table}


% \begin{figure}[h]
% 	\centering
% 	\includegraphics[scale=0.5]{../output/fast_2.pdf}
% 	\label{fig:fast_2}
% 	\caption{This figure shows the numeric solution for our specific algorithm and the exact solution on top (green dashed line). The step size used is $h=10^{-2}$.}
% \end{figure}

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[scale=0.5]{../output/fast_6.pdf}
% 	\label{fig:fast_6}
% 	\caption{This figure shows the numeric solution for our specific algorithm and the exact solution on top (green dashed line). The step size used is $h=10^{-6}$.}
% \end{figure}

\begin{table} [h]  %[p] % Uncomment to put table in Appendix
  \input{../output/fast_errors_tex.txt}
	\caption{This table shows the relative error for our specialized method. The firs column is the logarithm of our step size and the second column is the relative error (see figure \ref{fig:fast_error} for table plotted).}
	\label{tab:fast_error}
\end{table}


% \begin{figure}[h]
% 	\centering
% 	\includegraphics[scale=0.5]{../output/LU_2.pdf}
% 	\label{fig:lu_2}
% 	\caption{This figure shows the numeric solution for the LU-algorithm and the exact solution on top (green dashed line). The step size used is $h=10^{-2}$.}
% \end{figure}

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[scale=0.5]{../output/LU_4.pdf}
% 	\label{fig:lu_4}
% 	\caption{This figure shows the numeric solution for the LU-algorithm and the exact solution on top (green dashed line). The step size used is $h=10^{-4}$.}
% \end{figure}

\begin{table}  %[p] % Uncomment to put table in Appendix
  \input{../output/LU_errors_tex.txt}
  \caption{This table shows the relative error for our LU decomposition method. The first column is the logarithm of our step size and the second column is the relative error (see figure \ref{fig:lu_error} for table plotted).}
	\label{tab:lu_error}
\end{table}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{../output/slow_errors.pdf}
	\label{fig:slow_error}
	\caption{Here we have the table of errors (table \ref{tab:slow_error}) from our general method plotted. Along the x-axis we have the logarithm of the step size ($log_{10}(h)$) and along the y-axis we have the relative error ($\epsilon$).}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{../output/fast_errors.pdf}
	\label{fig:fast_error}
	\caption{Here we have the table of errors (table \ref{tab:fast_error}) from our specialized method plotted. Along the x-axis we have the logarithm of the step size ($log_{10}(h)$) and along the y-axis we have the relative error ($\epsilon$).}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{../output/LU_errors.pdf}
	\label{fig:lu_error}
	\caption{Here we have the table of errors (table \ref{tab:lu_error}) from the LU decomposition method plotted. Along the x-axis we have the logarithm of the step size ($log_{10}(h)$) and along the y-axis we have the relative error ($\epsilon$).}
\end{figure}

Table \ref{tab:time} shows the computational time for each of the algorithm for all step sizes.

\begin{table}  %[p] % Uncomment to put table in Appendix
  \input{../output/time_it_tex.txt}
  \caption{This table shows the time used for the different algorithms in microseconds. The first column is the number of steps used, second the general algorithm, third the specialized one and the third correspond to the LU decomposition.}
	\label{tab:time}
\end{table}



\section{Discussion}

Visually the plots does not change for different step sizes used, however looking at the relative error we can notice a change. For the first three step sizes $h = 10^{-2}, 10^{-3}, 10^{-4}$ we notice no difference between the methods and they decrease as expected. We notice a difference however for smaller step sizes.

The LU decomposition fails for $n=10^{-5}$. This is not surprising when we look at how the method works. For $n=10^{-5}$ we need to store $10^10$ numbers and each number uses eights bytes. This means we have to store 80Gb of data, where normal laptops usually have 8-24

-lu funker ikke for n=-5 tall tar opp en 8byte ikke nok ram
-


\section{Conclusion}

blabla



\onecolumngrid
\vspace{1cm} % some extra space
\newpage

\begin{thebibliography}{}
  \bibitem {armadillo} Armadillo, http://arma.sourceforge.net/
  \bibitem {oppgavetekst} Dept. of Physics, UiO,  2020, \textit{Project 1, deadline September 9}
  \bibitem {github} Rustad, S. and Falmår, V.,  2020, \textit{FYS3150 Project 1}, https://github.com/sigurdru/FYS3150/tree/master/Project1
  \bibitem {lu} Weisstein, Eric, W. on Mathworld--A Wolfram Web Resource, \textit{LU Decomposition}, read 13.09.20, https://mathworld.wolfram.com/LUDecomposition.html
  \bibitem {lu_wiki} Wikipedia.  2020, \textit{LU decomposition}, read 13.09.20, https://en.wikipedia.org/wiki/LU\_decomposition
\end{thebibliography}



\end{document}
